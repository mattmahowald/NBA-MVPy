{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tf_rnn_classifier import TfRNNClassifier\n",
    "import tweepy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "\n",
    "auth = tweepy.AppAuthHandler('aiA4EZjqeZ13it3Qvsgy9nHcv', 'J9sghg8mbgT7wF7vGSa6ZtvKpDaD9DdaCc1ljjTyF8QMNHK7Y8')\n",
    "# auth.set_access_token('1003792857709232128-hyBfditXJWmvTLKiVS1laz3ofjP88B', 'j5WX4xTr9nc8iyGgIiPVM78xlCUVNfffPsKgr8TvKwihQ')\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    num_tweets = 1000\n",
    "    files = ['data/positive_tweets.txt', 'data/negative_tweets.txt']\n",
    "    for file in files:\n",
    "        query = ''\n",
    "        if file == 'data/positive_tweets.txt': \n",
    "            query = ':) OR (: OR :-) OR <3 OR :D -:('\n",
    "        else:\n",
    "            query = ':( OR ): OR >:( -:)'\n",
    "        query +=' -filter:retweets' # ignore retweets\n",
    "        with open(file,'w') as f:\n",
    "            for tweet in tweepy.Cursor(api.search,q=query,tweet_mode=\"extended\").items(num_tweets):         \n",
    "                if tweet.lang == 'en':\n",
    "                    txt = tweet.full_text.replace('\\n', '')\n",
    "                    f.write(txt)\n",
    "                    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBAData(object):\n",
    "    def __init__(self):\n",
    "        self.path = 'data/'       \n",
    "    \n",
    "    # Call GATE POS tagger to label words accordingly\n",
    "    def tag_tweets(self, file):\n",
    "        # files in the tagger expect to be referenced from within tagger folder      \n",
    "        curr_path = os.getcwd()\n",
    "        path = '{}{}'.format(curr_path, '/twitie-tagger')\n",
    "        os.chdir(path)\n",
    "        # change tweets_pos/new_rigs\n",
    "        c = 'java -jar ./twitie_tag.jar ./models/gate-EN-twitter.model ../data/{} > ../data/tagged_{}'.format(file, file)\n",
    "        os.system(c)\n",
    "        os.chdir(curr_path)\n",
    "        \n",
    "\n",
    "    def bigrams_unigrams_phi(self, text):\n",
    "        words = ['<S>'] + text.split() + ['</S>']\n",
    "        bigrams = []\n",
    "        unigrams = []\n",
    "        for i in range(len(words) - 1):\n",
    "            if i != 0:\n",
    "                unigrams.append((words[i],)) \n",
    "            bigrams.append((words[i], words[i+1]))\n",
    "        return Counter(unigrams + bigrams)       \n",
    "\n",
    "    \n",
    "    # Featurizes data into POS bigrams after data has been cleaned, stripped, etc    \n",
    "    def transform_data(self, data, vectorizer=None):\n",
    "        dicts = {}\n",
    "        feat_matrix = None\n",
    "        feat_dicts = []\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            feat_dicts.append(self.bigrams_unigrams_phi(data[i]))\n",
    "        \n",
    "        vectorizer = DictVectorizer()    \n",
    "        feat_matrix = vectorizer.fit_transform(feat_dicts)        \n",
    "        \n",
    "        return {'X': feat_matrix, 'vectorizer': vectorizer}\n",
    "\n",
    "    # remove links, regularize capitalization, etc on all files passed in \n",
    "    def preprocess_data(self, file):\n",
    "        pos_emoji_pattern = r'(:-?(?:\\)+|D+))|((?:\\(+)-?:<?)|(<3+)'\n",
    "        neg_emoji_pattern = r'(>?:-?(?:\\(+))|((?:D+|\\)+)-?:<?)'\n",
    "        lines = None \n",
    "        with open('{}{}'.format(self.path, file), 'r') as f:\n",
    "            lines = f.readlines() # old contents with all the extra shit\n",
    "        with open('{}{}'.format(self.path, file), 'w') as f:        \n",
    "            for line in lines:\n",
    "                new_line = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', line, flags=re.MULTILINE).lower()\n",
    "                new_line = re.sub(pos_emoji_pattern, '', new_line)\n",
    "                new_line = re.sub(neg_emoji_pattern, '', new_line)\n",
    "                f.write(new_line)\n",
    "                    \n",
    "#     def get_labelled_data(self, file):\n",
    "#         data = []\n",
    "#         with open('{}{}'.format(self.path, dataset), 'r') as f:\n",
    "#             for line in f.readlines():\n",
    "#                 line = line.strip()\n",
    "#                 idx = line.rfind(',')\n",
    "#                 data.append((line[1:idx-1], int(line[idx+1:])))\n",
    "#         return data\n",
    "    \n",
    "    def get_processed_data(self, file):\n",
    "        data = []\n",
    "        with open('{}{}'.format(self.path, file), 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.strip()\n",
    "                data.append(line)\n",
    "        return data\n",
    "\n",
    "    def label_data(self, process_data):\n",
    "        pos_emoji_pattern = r'(:-?(?:\\)+|D+))|((?:\\(+)-?:<?)|(<3+)'\n",
    "        neg_emoji_pattern = r'(>?:-?(?:\\(+))|((?:D+|\\)+)-?:<?)'\n",
    "        data = []\n",
    "        for entry in process_data:\n",
    "            entry = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', entry, flags=re.MULTILINE)\n",
    "            sentiment = 0\n",
    "            if re.findall(pos_emoji_pattern, entry):\n",
    "                sentiment += 1\n",
    "                entry = re.sub(pos_emoji_pattern, '', entry)\n",
    "            if re.findall(neg_emoji_pattern, entry):\n",
    "                sentiment -= 1\n",
    "                entry = re.sub(neg_emoji_pattern, '', entry)\n",
    "            data.append((entry, sentiment))\n",
    "        return data\n",
    "            \n",
    "    def predict_baseline(self, data, labels):\n",
    "        correct = 0\n",
    "        attempts = 0\n",
    "        incorrect = [('tweet', 'exp_sentiment', 'sentiment')]\n",
    "        for i in range(len(data)):\n",
    "            tweet = data[i]\n",
    "            sentiment = labels[i]\n",
    "            exp_sentiment = self.get_tweet_sentiment_baseline(tweet)\n",
    "            if exp_sentiment == sentiment:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect.append((tweet, exp_sentiment, sentiment))\n",
    "            attempts += 1\n",
    "#         print ('{}% successful: {} correct, {} attempts'.format(correct*100//attempts, correct, attempts))\n",
    "        print(float(correct)/attempts)\n",
    "            \n",
    "    def get_tweet_sentiment_baseline(self, tweet):\n",
    "        analysis = TextBlob(self.clean_tweet(tweet))\n",
    "        # set sentiment\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 1\n",
    "        elif analysis.sentiment.polarity <= 0:\n",
    "            return -1\n",
    "\n",
    "    def predict(self, X, y, clf):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)        \n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        print('Accuracy: %0.03f' % accuracy_score(y_test, predictions))\n",
    "        print(classification_report(y_test, predictions, digits=3))\n",
    "        misclassified_samples = X_test[y_test != predictions]\n",
    "        print(misclassified_samples)\n",
    "        return clf\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "    \n",
    "    def get_vocab(self, data):\n",
    "        wc = Counter([w for d in data for w in d.split()])\n",
    "        wc = wc.items()\n",
    "        vocab = {w for w, c in wc}\n",
    "        vocab.add(\"$UNK\")\n",
    "        return sorted(vocab)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NBAData()\n",
    "files = ['positive_tweets.txt', 'negative_tweets.txt']\n",
    "for file in files: # probably want to go ahead and do this for all tweets about NBA players too... \n",
    "    model.preprocess_data(file)\n",
    "    model.tag_tweets(file) # new files w/ GATE POS labels for all relevant tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.618348623853211\n"
     ]
    }
   ],
   "source": [
    "# textblob shitty ass fuckin baseline you piece of shit rig \n",
    "model = NBAData()\n",
    "pos_data = model.get_processed_data('positive_tweets.txt') # list of processed tweets\n",
    "neg_data = model.get_processed_data('negative_tweets.txt')\n",
    "pos_labels = [1] * len(pos_data)\n",
    "neg_labels = [-1] * len(neg_data)\n",
    "\n",
    "model.predict_baseline(pos_data + neg_data, pos_labels + neg_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NBAData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 545\n",
      "Accuracy: 0.661\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1      0.565     0.848     0.678        46\n",
      "          1      0.825     0.524     0.641        63\n",
      "\n",
      "avg / total      0.715     0.661     0.657       109\n",
      "\n",
      "  (0, 398)\t1.0\n",
      "  (0, 399)\t1.0\n",
      "  (0, 539)\t1.0\n",
      "  (0, 1795)\t1.0\n",
      "  (0, 1796)\t1.0\n",
      "  (0, 2322)\t1.0\n",
      "  (0, 2323)\t1.0\n",
      "  (0, 2484)\t1.0\n",
      "  (0, 2486)\t1.0\n",
      "  (0, 2590)\t1.0\n",
      "  (0, 2591)\t1.0\n",
      "  (0, 2923)\t1.0\n",
      "  (0, 2934)\t1.0\n",
      "  (0, 3152)\t2.0\n",
      "  (0, 3154)\t1.0\n",
      "  (0, 3155)\t1.0\n",
      "  (0, 3206)\t1.0\n",
      "  (0, 3215)\t1.0\n",
      "  (0, 3395)\t1.0\n",
      "  (0, 3397)\t1.0\n",
      "  (0, 3973)\t1.0\n",
      "  (0, 3974)\t1.0\n",
      "  (0, 4045)\t1.0\n",
      "  (0, 4046)\t1.0\n",
      "  (0, 4870)\t1.0\n",
      "  :\t:\n",
      "  (35, 4514)\t1.0\n",
      "  (35, 9495)\t1.0\n",
      "  (35, 9496)\t1.0\n",
      "  (35, 11848)\t1.0\n",
      "  (35, 11851)\t1.0\n",
      "  (36, 950)\t1.0\n",
      "  (36, 2375)\t1.0\n",
      "  (36, 2378)\t1.0\n",
      "  (36, 2613)\t1.0\n",
      "  (36, 2639)\t1.0\n",
      "  (36, 3504)\t1.0\n",
      "  (36, 3539)\t1.0\n",
      "  (36, 4894)\t1.0\n",
      "  (36, 4895)\t1.0\n",
      "  (36, 6982)\t1.0\n",
      "  (36, 6983)\t1.0\n",
      "  (36, 7864)\t1.0\n",
      "  (36, 7865)\t1.0\n",
      "  (36, 9409)\t1.0\n",
      "  (36, 9467)\t1.0\n",
      "  (36, 9866)\t1.0\n",
      "  (36, 9867)\t1.0\n",
      "  (36, 10272)\t2.0\n",
      "  (36, 10273)\t1.0\n",
      "  (36, 10288)\t1.0\n",
      "Accuracy: 0.716\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1      0.667     0.652     0.659        46\n",
      "          1      0.750     0.762     0.756        63\n",
      "\n",
      "avg / total      0.715     0.716     0.715       109\n",
      "\n",
      "  (0, 398)\t1.0\n",
      "  (0, 399)\t1.0\n",
      "  (0, 539)\t1.0\n",
      "  (0, 1795)\t1.0\n",
      "  (0, 1796)\t1.0\n",
      "  (0, 2322)\t1.0\n",
      "  (0, 2323)\t1.0\n",
      "  (0, 2484)\t1.0\n",
      "  (0, 2486)\t1.0\n",
      "  (0, 2590)\t1.0\n",
      "  (0, 2591)\t1.0\n",
      "  (0, 2923)\t1.0\n",
      "  (0, 2934)\t1.0\n",
      "  (0, 3152)\t2.0\n",
      "  (0, 3154)\t1.0\n",
      "  (0, 3155)\t1.0\n",
      "  (0, 3206)\t1.0\n",
      "  (0, 3215)\t1.0\n",
      "  (0, 3395)\t1.0\n",
      "  (0, 3397)\t1.0\n",
      "  (0, 3973)\t1.0\n",
      "  (0, 3974)\t1.0\n",
      "  (0, 4045)\t1.0\n",
      "  (0, 4046)\t1.0\n",
      "  (0, 4870)\t1.0\n",
      "  :\t:\n",
      "  (29, 10453)\t1.0\n",
      "  (29, 10535)\t1.0\n",
      "  (29, 10587)\t1.0\n",
      "  (29, 11009)\t1.0\n",
      "  (29, 11018)\t1.0\n",
      "  (30, 950)\t1.0\n",
      "  (30, 2375)\t1.0\n",
      "  (30, 2378)\t1.0\n",
      "  (30, 2613)\t1.0\n",
      "  (30, 2639)\t1.0\n",
      "  (30, 3504)\t1.0\n",
      "  (30, 3539)\t1.0\n",
      "  (30, 4894)\t1.0\n",
      "  (30, 4895)\t1.0\n",
      "  (30, 6982)\t1.0\n",
      "  (30, 6983)\t1.0\n",
      "  (30, 7864)\t1.0\n",
      "  (30, 7865)\t1.0\n",
      "  (30, 9409)\t1.0\n",
      "  (30, 9467)\t1.0\n",
      "  (30, 9866)\t1.0\n",
      "  (30, 9867)\t1.0\n",
      "  (30, 10272)\t2.0\n",
      "  (30, 10273)\t1.0\n",
      "  (30, 10288)\t1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tagged_files = ['tagged_positive_tweets.txt', 'tagged_negative_tweets.txt']\n",
    "pos_data = model.get_processed_data('tagged_positive_tweets.txt') # list of processed tweets\n",
    "neg_data = model.get_processed_data('tagged_negative_tweets.txt')\n",
    "\n",
    "res = model.transform_data(pos_data + neg_data)\n",
    "X = res['X']\n",
    "pos_labels = [1] * len(pos_data)\n",
    "neg_labels = [-1] * len(neg_data)\n",
    "y = pos_labels + neg_labels\n",
    "print(\"Number of training samples: {}\".format(len(y)))\n",
    "\n",
    "nb_clf = model.predict(X, y, MultinomialNB(alpha=0.5))\n",
    "svc_clf = model.predict(X, y, LinearSVC())\n",
    "\n",
    "\n",
    "\n",
    "# tf_rnn = TfRNNClassifier(\n",
    "#     model.get_vocab(pos_data + neg_data),\n",
    "#     embed_dim=50,\n",
    "#     hidden_dim=50,\n",
    "#     max_length=200, # idk how long a tweet is\n",
    "#     hidden_activation=tf.nn.tanh,\n",
    "#     cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "#     train_embedding=True,\n",
    "#     max_iter=100,\n",
    "#     eta=0.05) \n",
    "\n",
    "# rnn_X = [[w for w in d.split()] for d in pos_data + neg_data]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(rnn_X, y, test_size=0.2, random_state=42) \n",
    "\n",
    "# tf_rnn.fit(X_train, y_train)\n",
    "# tf_rnn_dev_predictions = tf_rnn.predict(X_test)\n",
    "# print(classification_report(y_test, tf_rnn_dev_predictions))\n",
    "\n",
    "# print(accuracy_score(y_test, tf_rnn_dev_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
